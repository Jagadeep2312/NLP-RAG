{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7eb7378-d46f-4bcd-b11a-a669b3c4d04a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 1134\n",
      "Embedding shape: (1134, 1000)\n",
      "\n",
      "Sample chunk:\n",
      " 1 James ¬∑ Witten ¬∑ Hastie ¬∑ TibshiraniSpringer Texts in Statistics Gareth James ¬∑ Daniela Witten ¬∑ Trevor Hastie ¬∑ Robert Tibshirani An Introduction to Statistical Learning with Applications in R Springer Texts in Statistics An Introduction to Statistical Learning Gareth James Daniela Witten Trevor Hastie Robert Tibshirani Statistics An Introduction to Statistical Learning with Applications in R A\n",
      "\n",
      "Sample embedding (first 10 values):\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pdf_path = \"ML.pdf\"\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "raw_text = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text + \" \"\n",
    "\n",
    "clean_text = re.sub(r\"\\s+\", \" \", raw_text).strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=150):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(clean_text)\n",
    "\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "embeddings = vectorizer.fit_transform(chunks).toarray()\n",
    "\n",
    "print(\"Embedding shape:\", embeddings.shape)\n",
    "\n",
    "print(\"\\nSample chunk:\\n\", chunks[0][:400])\n",
    "print(\"\\nSample embedding (first 10 values):\\n\", embeddings[0][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ee102c-3c5e-44c0-91e6-4f1251257bc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 1134\n",
      "\n",
      "Top matching chunks:\n",
      "----------------------------------------\n",
      "Score: 0.6050338778401525\n",
      "any statistical learning methods. At the same time, there has been growing recognition across a number of Ô¨Åelds, from business to health care to genetics to the social sciences and beyond, that statistical learning is a powerful tool with important practical applications.Asaresult,theÔ¨Åeldhasmovedfromoneofprimarilyacademic interest to a mainstream discipline,with an enormous potential audience. Thi\n",
      "----------------------------------------\n",
      "Score: 0.6002354377713223\n",
      "tion An Overview of Statistical Learning Statistical learningrefersto avastsetoftoolsfor understanding data.T h e s e tools can be classiÔ¨Åed as supervised or unsupervised. Broadly speaking, supervised statistical learning involves building a statistical model for pre- dicting, or estimating, anoutput b a s e do no n eo rm o r einputs.P r o b l e m so f this natureoccurinÔ¨Åelds asdiverseasbusiness,m\n",
      "----------------------------------------\n",
      "Score: 0.5898873441772416\n",
      "s and 8 To our parents: Alison and Michael James Chiara Nappi and Edward Witten Valerie and Patrick Hastie Vera and Sami Tibshirani and to our families: Michael, Daniel, and Catherine Tessa,Theo, and Ari Samantha, Timothy, and Lynda Charlie, Ryan, Julie, and Cheryl Preface Statistical learning refers to a set of tools for modeling and understanding complex datasets. It is a recently developed area\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "pdf_path = \"ML.pdf\"\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "raw_text = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text + \" \"\n",
    "\n",
    "clean_text = re.sub(r\"\\s+\", \" \", raw_text).strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=150):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(clean_text)\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(chunks)\n",
    "\n",
    "def search(query, top_k=3):\n",
    "    q_vec = vectorizer.transform([query])\n",
    "    scores = cosine_similarity(q_vec, X)[0]\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "    print(\"\\nTop matching chunks:\")\n",
    "    for i in top_indices:\n",
    "        print(\"-\" * 40)\n",
    "        print(\"Score:\", scores[i])\n",
    "        print(chunks[i][:400])\n",
    "\n",
    "search(\"What is statistical learning?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee24aaa8-a81c-4887-be77-42cdd9e16777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 1134\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Enter your question:  Explain bias variance tradeoff ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Retrieved Relevant Sections:\n",
      "\n",
      "[Section 1]  (Score: 0.4160)\n",
      "ndicates the Ô¨Çexibility level corresponding to the smallest test MSE. the truef is very non-linear. There is also very little increase in variance as Ô¨Çexibility increases. Consequently, the test MSE declines substantially before experiencing a small increase as model Ô¨Çexibility increases. The relationship between bias, variance, and test set MSE given in Equa- tion 2.7 and displayed in Figure 2.12\n",
      "--------------------------------------------------\n",
      "[Section 2]  (Score: 0.3688)\n",
      "ntal line to the data). The challenge lies in Ô¨Ånding a method for which both the variance and the squared bias are low. This trade-oÔ¨Ä is one of the most important recurring themes in this book. In a real-life situation in whichf is unobserved, it is generally not pos- sible to explicitly compute the test MSE, bias, or variance for a statistical learning method. Nevertheless, one should always keep\n",
      "--------------------------------------------------\n",
      "[Section 3]  (Score: 0.3225)\n",
      " In otherwords, linear regression results in high bias in this example. However, in Figure 2.10 the truef is very close to linear, and so given enough data, it should be possible for linear regression to produce an accurateestimate. Generally, more Ô¨Çexible methods result in less bias. As a general rule, as we use more Ô¨Çexible methods, the variance will increase and the bias will decrease. The rela\n",
      "--------------------------------------------------\n",
      "\n",
      "ü§ñ Simplified Answer:\n",
      "\n",
      "ndicates the Ô¨Çexibility level corresponding to the smallest test MSE. the truef is very non-linear. There is also very little increase in variance as Ô¨Çexibility increases. Consequently, the test MSE declines substantially before experiencing a small increase as model Ô¨Çexibility increases. The relationship between bias, variance, and test set MSE given in Equa- tion 2.7 and displayed in Figure 2.12 is referred to as thebias-variance trade-oÔ¨Ä.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "pdf_path = \"ML.pdf\"\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "raw_text = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text + \" \"\n",
    "\n",
    "clean_text = re.sub(r\"\\s+\", \" \", raw_text).strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=150):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(clean_text)\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(chunks)\n",
    "\n",
    "\n",
    "question = input(\"\\n‚ùì Enter your question: \")\n",
    "\n",
    "query_vector = vectorizer.transform([question])\n",
    "\n",
    "scores = cosine_similarity(query_vector, X)[0]\n",
    "top_k = 3\n",
    "top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "retrieved_chunks = [chunks[i] for i in top_indices]\n",
    "\n",
    "print(\"\\nüîç Retrieved Relevant Sections:\\n\")\n",
    "for i, idx in enumerate(top_indices, 1):\n",
    "    print(f\"[Section {i}]  (Score: {scores[idx]:.4f})\")\n",
    "    print(chunks[idx][:400])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def simplify_text(text, max_sentences=5):\n",
    "    sentences = text.split(\". \")\n",
    "    return \". \".join(sentences[:max_sentences]) + \".\"\n",
    "\n",
    "combined_text = \" \".join(retrieved_chunks)\n",
    "\n",
    "print(\"\\nü§ñ Simplified Answer:\\n\")\n",
    "print(simplify_text(combined_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e030d5fb-96e1-4418-9be4-3686c3df9ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 1134\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Enter your question:  What is dimensionality reduction?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Retrieved Relevant Sections:\n",
      "\n",
      "[Section 1]  (Score: 0.3196)\n",
      "opulation size (pop) and ad spending (ad)f o r100 diÔ¨Äerent cities are shown as purple circles. The green solid line indicates the Ô¨Årst principal component, and the blue dashed line indicates the second principal component. where Œ≤j = M‚àë m=1 Œ∏mœÜjm. (6.18) Hence (6.17) can be thought of as a special case of the original linear regression model given by (6.1). Dimension reduction serves to constrain \n",
      "--------------------------------------------------\n",
      "[Section 2]  (Score: 0.2979)\n",
      "6.17) is equivalent to performing least squares on the originalp predictors. All dimension reduction methods work in two steps. First, the trans- formed predictors Z1,Z2,...,Z M are obtained. Second, the model is Ô¨Åt using theseM predictors. However, the choice ofZ1,Z2,...,Z M, or equiv- alently, the selection of theœÜjm‚Äôs, can be achieved in diÔ¨Äerent ways. In this chapter, we will considertwo appro\n",
      "--------------------------------------------------\n",
      "[Section 3]  (Score: 0.2216)\n",
      "n (6.17), the regression coeÔ¨Écients are given byŒ∏0,Œ∏1,...,Œ∏ M. Ifthe constantsœÜ1m,œÜ2m,...,œÜ pm arechosenwisely, then such dimension reduction approaches can often outperform least squares regression. In other words, Ô¨Åtting (6.17) using least squares can lead to better results than Ô¨Åtting (6.1) using least squares. The term dimension reduction comes from the fact that this approach reduces the prob\n",
      "--------------------------------------------------\n",
      "\n",
      "ü§ñ Simplified Answer:\n",
      "\n",
      "opulation size (pop) and ad spending (ad)f o r100 diÔ¨Äerent cities are shown as purple circles. The green solid line indicates the Ô¨Årst principal component, and the blue dashed line indicates the second principal component. where Œ≤j = M‚àë m=1 Œ∏mœÜjm. (6.18) Hence (6.17) can be thought of as a special case of the original linear regression model given by (6.1). Dimension reduction serves to constrain the estimated Œ≤j coeÔ¨Écients, since now they must take the form (6.18).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "pdf_path = \"ML.pdf\"\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "raw_text = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text + \" \"\n",
    "\n",
    "clean_text = re.sub(r\"\\s+\", \" \", raw_text).strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=150):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(clean_text)\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(chunks)\n",
    "\n",
    "\n",
    "question = input(\"\\n‚ùì Enter your question: \")\n",
    "\n",
    "query_vector = vectorizer.transform([question])\n",
    "\n",
    "scores = cosine_similarity(query_vector, X)[0]\n",
    "top_k = 3\n",
    "top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "retrieved_chunks = [chunks[i] for i in top_indices]\n",
    "\n",
    "print(\"\\nüîç Retrieved Relevant Sections:\\n\")\n",
    "for i, idx in enumerate(top_indices, 1):\n",
    "    print(f\"[Section {i}]  (Score: {scores[idx]:.4f})\")\n",
    "    print(chunks[idx][:400])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def simplify_text(text, max_sentences=5):\n",
    "    sentences = text.split(\". \")\n",
    "    return \". \".join(sentences[:max_sentences]) + \".\"\n",
    "\n",
    "combined_text = \" \".join(retrieved_chunks)\n",
    "\n",
    "print(\"\\nü§ñ Simplified Answer:\\n\")\n",
    "print(simplify_text(combined_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5debeef-53d9-416d-a770-f5d30bd06abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 1134\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Enter your question:  Define principal components ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Retrieved Relevant Sections:\n",
      "\n",
      "[Section 1]  (Score: 0.8423)\n",
      "re enough is inherently ill-deÔ¨Åned, and will depend on the speciÔ¨Åc area of application and the speciÔ¨Åc data set. In practice, we tend to look at the Ô¨Årst few principal components in order to Ô¨Ånd interesting patterns in the data. If no interesting patterns are found in the Ô¨Årst few principal components, then further principal components are unlikely to be of inter- est. Conversely, if the Ô¨Årst few \n",
      "--------------------------------------------------\n",
      "[Section 2]  (Score: 0.8250)\n",
      "ond principal component and these two predictors, again suggesting that in this case, one only needs the Ô¨Årst principal component in order to accurately represent thepop and ad budgets. With two-dimensional data, such as in our advertising example, we can construct at most two principal components. However, if we had other predictors, such as population age, income level, education, and so forth, \n",
      "--------------------------------------------------\n",
      "[Section 3]  (Score: 0.7761)\n",
      "omponents Analysis Principal components are discussed in Section 6.3.1 in the context of principal components regression. When faced with a large set of corre- lated variables, principal components allow us to summarize this set with a smaller number of representative variables that collectively explain most of the variability in the original set. The principal component directions are presented i\n",
      "--------------------------------------------------\n",
      "\n",
      "ü§ñ Simplified Answer:\n",
      "\n",
      "re enough is inherently ill-deÔ¨Åned, and will depend on the speciÔ¨Åc area of application and the speciÔ¨Åc data set. In practice, we tend to look at the Ô¨Årst few principal components in order to Ô¨Ånd interesting patterns in the data. If no interesting patterns are found in the Ô¨Årst few principal components, then further principal components are unlikely to be of inter- est. Conversely, if the Ô¨Årst few principal components are interesting, then we typically continue to look at subsequent principal components until no further interesting patterns are found. This is admittedly a subjective ap- proach, and is reÔ¨Çective of the fact that PCA is generally used as a tool for exploratory data analysis.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "pdf_path = \"ML.pdf\"\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "raw_text = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text + \" \"\n",
    "\n",
    "clean_text = re.sub(r\"\\s+\", \" \", raw_text).strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=150):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(clean_text)\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(chunks)\n",
    "\n",
    "\n",
    "question = input(\"\\n‚ùì Enter your question: \")\n",
    "\n",
    "query_vector = vectorizer.transform([question])\n",
    "\n",
    "scores = cosine_similarity(query_vector, X)[0]\n",
    "top_k = 3\n",
    "top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "retrieved_chunks = [chunks[i] for i in top_indices]\n",
    "\n",
    "print(\"\\nüîç Retrieved Relevant Sections:\\n\")\n",
    "for i, idx in enumerate(top_indices, 1):\n",
    "    print(f\"[Section {i}]  (Score: {scores[idx]:.4f})\")\n",
    "    print(chunks[idx][:400])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def simplify_text(text, max_sentences=5):\n",
    "    sentences = text.split(\". \")\n",
    "    return \". \".join(sentences[:max_sentences]) + \".\"\n",
    "\n",
    "combined_text = \" \".join(retrieved_chunks)\n",
    "\n",
    "print(\"\\nü§ñ Simplified Answer:\\n\")\n",
    "print(simplify_text(combined_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d892fb49-43f1-4c06-970f-c6cb4f858409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 1134\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Enter your question:  What is the use of cross validation?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Retrieved Relevant Sections:\n",
      "\n",
      "[Section 1]  (Score: 0.5521)\n",
      "te resulting from k-fold CV. To summarize, there is a bias-variance trade-oÔ¨Ä associated with the choice ofk in k-fold cross-validation. Typically, given these considerations, one performsk-fold cross-validation usingk =5o r k = 10, as these values have been shown empirically to yield test error rate estimates that suÔ¨Äer neither from excessively high bias nor from very high variance. 5.1.5 Cross-Va\n",
      "--------------------------------------------------\n",
      "[Section 2]  (Score: 0.5432)\n",
      "included in the validation set. 2. In the validation approach, only a subset of the observations‚Äîthose that are included in the training set rather than in the validation set‚Äîare used to Ô¨Åt the model. Since statistical methods tend to per- form worse when trained on fewer observations, this suggests that the validation set error rate may tend tooverestimate the test error rate for the model Ô¨Åt on \n",
      "--------------------------------------------------\n",
      "[Section 3]  (Score: 0.5024)\n",
      "N e i g h b o r s.................. 1 6 3 4.6.6 An Application to Caravan Insurance Data . . . . . 165 4 . 7 E x e r c i s e s ............................ 1 6 8 5 Resampling Methods 175 5 . 1 C r o s s - V a l i d a t i o n ........................ 1 7 6 5 . 1 . 1 T h eV a l i d a t i o nS e tA p p r o a c h............. 1 7 6 5.1.2 Leave-One-Out Cross-Validation . . . . . . . . . . . 178 5.1.3 k\n",
      "--------------------------------------------------\n",
      "\n",
      "ü§ñ Simplified Answer:\n",
      "\n",
      "te resulting from k-fold CV. To summarize, there is a bias-variance trade-oÔ¨Ä associated with the choice ofk in k-fold cross-validation. Typically, given these considerations, one performsk-fold cross-validation usingk =5o r k = 10, as these values have been shown empirically to yield test error rate estimates that suÔ¨Äer neither from excessively high bias nor from very high variance. 5.1.5 Cross-Validation on ClassiÔ¨Åcation Problems In this chapter so far, we have illustrated the use of cross-validation in the regression setting where the outcomeY is quantitative, and so have used MSE to quantify test error. But cross-validation can also be a very useful approach in the classiÔ¨Åcation setting whenY is qualitative.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "pdf_path = \"ML.pdf\"\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "raw_text = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text + \" \"\n",
    "\n",
    "clean_text = re.sub(r\"\\s+\", \" \", raw_text).strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=150):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(clean_text)\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(chunks)\n",
    "\n",
    "\n",
    "question = input(\"\\n‚ùì Enter your question: \")\n",
    "\n",
    "query_vector = vectorizer.transform([question])\n",
    "\n",
    "scores = cosine_similarity(query_vector, X)[0]\n",
    "top_k = 3\n",
    "top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "retrieved_chunks = [chunks[i] for i in top_indices]\n",
    "\n",
    "print(\"\\nüîç Retrieved Relevant Sections:\\n\")\n",
    "for i, idx in enumerate(top_indices, 1):\n",
    "    print(f\"[Section {i}]  (Score: {scores[idx]:.4f})\")\n",
    "    print(chunks[idx][:400])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def simplify_text(text, max_sentences=5):\n",
    "    sentences = text.split(\". \")\n",
    "    return \". \".join(sentences[:max_sentences]) + \".\"\n",
    "\n",
    "combined_text = \" \".join(retrieved_chunks)\n",
    "\n",
    "print(\"\\nü§ñ Simplified Answer:\\n\")\n",
    "print(simplify_text(combined_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3ede58f-768c-44d1-8acd-9600235977e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 1134\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Enter your question:  What is linear regression?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Retrieved Relevant Sections:\n",
      "\n",
      "[Section 1]  (Score: 0.4681)\n",
      "ns. In a real life situation in which the true relationship is unknown, one might drawthe conclusion that KNN should be favored over linear regression because it will at worst be slightly inferior than linear regression if the true relationship is linear, and may give substantially better results if the true relationship is non-linear. But in reality, even when the true relationship is highly non-\n",
      "--------------------------------------------------\n",
      "[Section 2]  (Score: 0.4159)\n",
      " blue curve. The linear regression Ô¨Åt for a model that includes all polynomials ofhorsepower up to Ô¨Åfth-degree is shown in green. orange line represents the linear regression Ô¨Åt. There is a pronounced rela- tionship betweenmpg and horsepower, but it seems clear that this relation- ship is in fact non-linear: the data suggest a curved relationship. A simple approach for incorporating non-linear ass\n",
      "--------------------------------------------------\n",
      "[Section 3]  (Score: 0.4142)\n",
      " linear regression to this binary response, and predict drug overdose if ÀÜY> 0.5a n dstroke otherwise. In the binary case it is not hard to show that even if we Ô¨Çip the above coding, linear regression will produce the same Ô¨Ånal predictions. For a binary response with a 0/1 coding as above, regression by least squares does make sense; it can be shownthat theXÀÜŒ≤ obtained using linear regression is i\n",
      "--------------------------------------------------\n",
      "\n",
      "ü§ñ Simplified Answer:\n",
      "\n",
      "ns. In a real life situation in which the true relationship is unknown, one might drawthe conclusion that KNN should be favored over linear regression because it will at worst be slightly inferior than linear regression if the true relationship is linear, and may give substantially better results if the true relationship is non-linear. But in reality, even when the true relationship is highly non-linear, KNN may still provide inferior results to linear regression. In particular, both Figures 3.18 and 3.19 illustrate settings withp = 1 predictor. But in higher dimensions, KNN often performs worse than linear regression.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "pdf_path = \"ML.pdf\"\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "raw_text = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text + \" \"\n",
    "\n",
    "clean_text = re.sub(r\"\\s+\", \" \", raw_text).strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=150):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(clean_text)\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(chunks)\n",
    "\n",
    "\n",
    "question = input(\"\\n‚ùì Enter your question: \")\n",
    "\n",
    "query_vector = vectorizer.transform([question])\n",
    "\n",
    "scores = cosine_similarity(query_vector, X)[0]\n",
    "top_k = 3\n",
    "top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "retrieved_chunks = [chunks[i] for i in top_indices]\n",
    "\n",
    "print(\"\\nüîç Retrieved Relevant Sections:\\n\")\n",
    "for i, idx in enumerate(top_indices, 1):\n",
    "    print(f\"[Section {i}]  (Score: {scores[idx]:.4f})\")\n",
    "    print(chunks[idx][:400])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def simplify_text(text, max_sentences=5):\n",
    "    sentences = text.split(\". \")\n",
    "    return \". \".join(sentences[:max_sentences]) + \".\"\n",
    "\n",
    "combined_text = \" \".join(retrieved_chunks)\n",
    "\n",
    "print(\"\\nü§ñ Simplified Answer:\\n\")\n",
    "print(simplify_text(combined_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2242f37d-e94a-440e-8238-d70210db9252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 1134\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Enter your question:  Explain regression in statistical learning?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Retrieved Relevant Sections:\n",
      "\n",
      "[Section 1]  (Score: 0.4230)\n",
      "s and 8 To our parents: Alison and Michael James Chiara Nappi and Edward Witten Valerie and Patrick Hastie Vera and Sami Tibshirani and to our families: Michael, Daniel, and Catherine Tessa,Theo, and Ari Samantha, Timothy, and Lynda Charlie, Ryan, Julie, and Cheryl Preface Statistical learning refers to a set of tools for modeling and understanding complex datasets. It is a recently developed area\n",
      "--------------------------------------------------\n",
      "[Section 2]  (Score: 0.3999)\n",
      "any statistical learning methods. At the same time, there has been growing recognition across a number of Ô¨Åelds, from business to health care to genetics to the social sciences and beyond, that statistical learning is a powerful tool with important practical applications.Asaresult,theÔ¨Åeldhasmovedfromoneofprimarilyacademic interest to a mainstream discipline,with an enormous potential audience. Thi\n",
      "--------------------------------------------------\n",
      "[Section 3]  (Score: 0.3967)\n",
      "tion An Overview of Statistical Learning Statistical learningrefersto avastsetoftoolsfor understanding data.T h e s e tools can be classiÔ¨Åed as supervised or unsupervised. Broadly speaking, supervised statistical learning involves building a statistical model for pre- dicting, or estimating, anoutput b a s e do no n eo rm o r einputs.P r o b l e m so f this natureoccurinÔ¨Åelds asdiverseasbusiness,m\n",
      "--------------------------------------------------\n",
      "\n",
      "ü§ñ Simplified Answer:\n",
      "\n",
      "s and 8 To our parents: Alison and Michael James Chiara Nappi and Edward Witten Valerie and Patrick Hastie Vera and Sami Tibshirani and to our families: Michael, Daniel, and Catherine Tessa,Theo, and Ari Samantha, Timothy, and Lynda Charlie, Ryan, Julie, and Cheryl Preface Statistical learning refers to a set of tools for modeling and understanding complex datasets. It is a recently developed area in statistics and blends with parallel developments in computer science and, in particular, machine learning. The Ô¨Åeld encompasses many methods such as the lasso and sparse regression, classiÔ¨Åcation and regression trees, and boosting and support vector machines. With the explosion of ‚ÄúBig Data‚Äù problems, statistical learning has be- come a very hot Ô¨Åeld in many scientiÔ¨Åc areas as well as marketing, Ô¨Ånance, and other business disciplines. People with statistical learning skills are in high demand.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "pdf_path = \"ML.pdf\"\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "raw_text = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text + \" \"\n",
    "\n",
    "clean_text = re.sub(r\"\\s+\", \" \", raw_text).strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=150):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(clean_text)\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(chunks)\n",
    "\n",
    "\n",
    "question = input(\"\\n‚ùì Enter your question: \")\n",
    "\n",
    "query_vector = vectorizer.transform([question])\n",
    "\n",
    "scores = cosine_similarity(query_vector, X)[0]\n",
    "top_k = 3\n",
    "top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "retrieved_chunks = [chunks[i] for i in top_indices]\n",
    "\n",
    "print(\"\\nüîç Retrieved Relevant Sections:\\n\")\n",
    "for i, idx in enumerate(top_indices, 1):\n",
    "    print(f\"[Section {i}]  (Score: {scores[idx]:.4f})\")\n",
    "    print(chunks[idx][:400])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def simplify_text(text, max_sentences=5):\n",
    "    sentences = text.split(\". \")\n",
    "    return \". \".join(sentences[:max_sentences]) + \".\"\n",
    "\n",
    "combined_text = \" \".join(retrieved_chunks)\n",
    "\n",
    "print(\"\\nü§ñ Simplified Answer:\\n\")\n",
    "print(simplify_text(combined_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64d22079-629d-42aa-a237-2e6d2956bf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 1134\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Enter your question:  What is classification? Explain with example?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Retrieved Relevant Sections:\n",
      "\n",
      "[Section 1]  (Score: 0.2431)\n",
      "s in the world stockmarkets.Hencewecollectweeklydataforallof2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market. 2.4 Exercises 53 the amount of Ô¨Çexibility in the method, and they-axis should represent the values for each curve. There should be Ô¨Åve curves. Make sure to label each one. (b\n",
      "--------------------------------------------------\n",
      "[Section 2]  (Score: 0.2182)\n",
      "] 11.5 In this case,usingŒª =0 .2leadstoaslightlylowertest MSEthan Œª =0 .001. 332 8. Tree-Based Methods 8.4 Exercises Conceptual 1. Draw an example (of your own invention) of a partition of two- dimensional feature space that could result from recursive binary splitting. Your example should contain at least six regions. Draw a decision tree corresponding to this partition. Be sure to label all as- \n",
      "--------------------------------------------------\n",
      "[Section 3]  (Score: 0.2126)\n",
      "predictors, perform forward stepwise selection on the training set in order to identify a satisfactory model that uses just a subset of the predictors. (b) Fit a GAM on the training data, using out-of-state tuition as the response and the features selected in the previous step as the predictors. Plot the results, and explain your Ô¨Åndings. (c) Evaluate the model obtained on the test set, and explai\n",
      "--------------------------------------------------\n",
      "\n",
      "ü§ñ Simplified Answer:\n",
      "\n",
      "s in the world stockmarkets.Hencewecollectweeklydataforallof2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market. 2.4 Exercises 53 the amount of Ô¨Çexibility in the method, and they-axis should represent the values for each curve. There should be Ô¨Åve curves. Make sure to label each one.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "pdf_path = \"ML.pdf\"\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "raw_text = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text + \" \"\n",
    "\n",
    "clean_text = re.sub(r\"\\s+\", \" \", raw_text).strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=150):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(clean_text)\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(chunks)\n",
    "\n",
    "\n",
    "question = input(\"\\n‚ùì Enter your question: \")\n",
    "\n",
    "query_vector = vectorizer.transform([question])\n",
    "\n",
    "scores = cosine_similarity(query_vector, X)[0]\n",
    "top_k = 3\n",
    "top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "retrieved_chunks = [chunks[i] for i in top_indices]\n",
    "\n",
    "print(\"\\nüîç Retrieved Relevant Sections:\\n\")\n",
    "for i, idx in enumerate(top_indices, 1):\n",
    "    print(f\"[Section {i}]  (Score: {scores[idx]:.4f})\")\n",
    "    print(chunks[idx][:400])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def simplify_text(text, max_sentences=5):\n",
    "    sentences = text.split(\". \")\n",
    "    return \". \".join(sentences[:max_sentences]) + \".\"\n",
    "\n",
    "combined_text = \" \".join(retrieved_chunks)\n",
    "\n",
    "print(\"\\nü§ñ Simplified Answer:\\n\")\n",
    "print(simplify_text(combined_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bff855a-329f-4837-8512-001a862a338f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 1134\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Enter your question:  Differentiate between supervised and unsupervised learning?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Retrieved Relevant Sections:\n",
      "\n",
      "[Section 1]  (Score: 0.6159)\n",
      "for the algorithm is that it predict accurately‚Äî interpretability is not a concern. In this setting, we might expect that it will be best to use the most Ô¨Çexible model available. Surprisingly, this is not always the case! We will often obtain more accurate predictions using a less Ô¨Çexible method. This phenomenon, which may seem counterintuitive at Ô¨Årst glance, has to do with the potential for over\n",
      "--------------------------------------------------\n",
      "[Section 2]  (Score: 0.5819)\n",
      "ociated response variableY. Rather, the goal is to discover interesting things about the measurements on X1,X2,...,X p. Is there an informative way to visualize the data? Can we discover subgroups among the variables or among the observations? Unsupervised learning refers to a diverse set of techniques for answering questions such as these. In this chapter, we will focus on two particu- lar types \n",
      "--------------------------------------------------\n",
      "[Section 3]  (Score: 0.5655)\n",
      "ches in Chapter 10. Many problems fall naturally into the supervised or unsupervised learn- ing paradigms. However, sometimes the question of whether an analysis should be considered supervised or unsupervised is less clear-cut. For in- stance, suppose that we have a set ofn observations. Form of the observa- tions, wherem<n , we have both predictor measurements and a response measurement. For the\n",
      "--------------------------------------------------\n",
      "\n",
      "ü§ñ Simplified Answer:\n",
      "\n",
      "for the algorithm is that it predict accurately‚Äî interpretability is not a concern. In this setting, we might expect that it will be best to use the most Ô¨Çexible model available. Surprisingly, this is not always the case! We will often obtain more accurate predictions using a less Ô¨Çexible method. This phenomenon, which may seem counterintuitive at Ô¨Årst glance, has to do with the potential for overÔ¨Åtting in highly Ô¨Çexible methods. We saw an example of overÔ¨Åtting in Figure 2.6.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "pdf_path = \"ML.pdf\"\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "raw_text = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text + \" \"\n",
    "\n",
    "clean_text = re.sub(r\"\\s+\", \" \", raw_text).strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=150):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(clean_text)\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(chunks)\n",
    "\n",
    "\n",
    "question = input(\"\\n‚ùì Enter your question: \")\n",
    "\n",
    "query_vector = vectorizer.transform([question])\n",
    "\n",
    "scores = cosine_similarity(query_vector, X)[0]\n",
    "top_k = 3\n",
    "top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "retrieved_chunks = [chunks[i] for i in top_indices]\n",
    "\n",
    "print(\"\\nüîç Retrieved Relevant Sections:\\n\")\n",
    "for i, idx in enumerate(top_indices, 1):\n",
    "    print(f\"[Section {i}]  (Score: {scores[idx]:.4f})\")\n",
    "    print(chunks[idx][:400])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def simplify_text(text, max_sentences=5):\n",
    "    sentences = text.split(\". \")\n",
    "    return \". \".join(sentences[:max_sentences]) + \".\"\n",
    "\n",
    "combined_text = \" \".join(retrieved_chunks)\n",
    "\n",
    "print(\"\\nü§ñ Simplified Answer:\\n\")\n",
    "print(simplify_text(combined_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2503a754-f594-4fc3-b5e2-d0d2b5a38138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 1134\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Enter your question:  Define supervised learning?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Retrieved Relevant Sections:\n",
      "\n",
      "[Section 1]  (Score: 0.5342)\n",
      "ches in Chapter 10. Many problems fall naturally into the supervised or unsupervised learn- ing paradigms. However, sometimes the question of whether an analysis should be considered supervised or unsupervised is less clear-cut. For in- stance, suppose that we have a set ofn observations. Form of the observa- tions, wherem<n , we have both predictor measurements and a response measurement. For the\n",
      "--------------------------------------------------\n",
      "[Section 2]  (Score: 0.5320)\n",
      "for the algorithm is that it predict accurately‚Äî interpretability is not a concern. In this setting, we might expect that it will be best to use the most Ô¨Çexible model available. Surprisingly, this is not always the case! We will often obtain more accurate predictions using a less Ô¨Çexible method. This phenomenon, which may seem counterintuitive at Ô¨Årst glance, has to do with the potential for over\n",
      "--------------------------------------------------\n",
      "[Section 3]  (Score: 0.4578)\n",
      "ociated response variableY. Rather, the goal is to discover interesting things about the measurements on X1,X2,...,X p. Is there an informative way to visualize the data? Can we discover subgroups among the variables or among the observations? Unsupervised learning refers to a diverse set of techniques for answering questions such as these. In this chapter, we will focus on two particu- lar types \n",
      "--------------------------------------------------\n",
      "\n",
      "ü§ñ Simplified Answer:\n",
      "\n",
      "ches in Chapter 10. Many problems fall naturally into the supervised or unsupervised learn- ing paradigms. However, sometimes the question of whether an analysis should be considered supervised or unsupervised is less clear-cut. For in- stance, suppose that we have a set ofn observations. Form of the observa- tions, wherem<n , we have both predictor measurements and a response measurement.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "pdf_path = \"ML.pdf\"\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "raw_text = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text + \" \"\n",
    "\n",
    "clean_text = re.sub(r\"\\s+\", \" \", raw_text).strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=150):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(clean_text)\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(chunks)\n",
    "\n",
    "\n",
    "question = input(\"\\n‚ùì Enter your question: \")\n",
    "\n",
    "query_vector = vectorizer.transform([question])\n",
    "\n",
    "scores = cosine_similarity(query_vector, X)[0]\n",
    "top_k = 3\n",
    "top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "retrieved_chunks = [chunks[i] for i in top_indices]\n",
    "\n",
    "print(\"\\nüîç Retrieved Relevant Sections:\\n\")\n",
    "for i, idx in enumerate(top_indices, 1):\n",
    "    print(f\"[Section {i}]  (Score: {scores[idx]:.4f})\")\n",
    "    print(chunks[idx][:400])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def simplify_text(text, max_sentences=5):\n",
    "    sentences = text.split(\". \")\n",
    "    return \". \".join(sentences[:max_sentences]) + \".\"\n",
    "\n",
    "combined_text = \" \".join(retrieved_chunks)\n",
    "\n",
    "print(\"\\nü§ñ Simplified Answer:\\n\")\n",
    "print(simplify_text(combined_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955c746e-11e5-4835-be30-660002d8da0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 1134\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Enter your question:  What is statistical learning?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Retrieved Relevant Sections:\n",
      "\n",
      "[Section 1]  (Score: 0.6050)\n",
      "any statistical learning methods. At the same time, there has been growing recognition across a number of Ô¨Åelds, from business to health care to genetics to the social sciences and beyond, that statistical learning is a powerful tool with important practical applications.Asaresult,theÔ¨Åeldhasmovedfromoneofprimarilyacademic interest to a mainstream discipline,with an enormous potential audience. Thi\n",
      "--------------------------------------------------\n",
      "[Section 2]  (Score: 0.6002)\n",
      "tion An Overview of Statistical Learning Statistical learningrefersto avastsetoftoolsfor understanding data.T h e s e tools can be classiÔ¨Åed as supervised or unsupervised. Broadly speaking, supervised statistical learning involves building a statistical model for pre- dicting, or estimating, anoutput b a s e do no n eo rm o r einputs.P r o b l e m so f this natureoccurinÔ¨Åelds asdiverseasbusiness,m\n",
      "--------------------------------------------------\n",
      "[Section 3]  (Score: 0.5899)\n",
      "s and 8 To our parents: Alison and Michael James Chiara Nappi and Edward Witten Valerie and Patrick Hastie Vera and Sami Tibshirani and to our families: Michael, Daniel, and Catherine Tessa,Theo, and Ari Samantha, Timothy, and Lynda Charlie, Ryan, Julie, and Cheryl Preface Statistical learning refers to a set of tools for modeling and understanding complex datasets. It is a recently developed area\n",
      "--------------------------------------------------\n",
      "\n",
      "ü§ñ Simplified Answer:\n",
      "\n",
      "any statistical learning methods. At the same time, there has been growing recognition across a number of Ô¨Åelds, from business to health care to genetics to the social sciences and beyond, that statistical learning is a powerful tool with important practical applications.Asaresult,theÔ¨Åeldhasmovedfromoneofprimarilyacademic interest to a mainstream discipline,with an enormous potential audience. This trend will surely continue with the increasing availability of enormous quantities of data and the software to analyze it. The purpose ofAn Introduction to Statistical Learning(ISL) is to facili- tate the transition of statistical learning from an academic to a mainstream Ô¨Åeld. ISL is not intended to replace ESL, which is a far more comprehen- sive text both in terms of the number of approaches considered and the depth to which they are explored.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "pdf_path = \"ML.pdf\"\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "raw_text = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text + \" \"\n",
    "\n",
    "clean_text = re.sub(r\"\\s+\", \" \", raw_text).strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, overlap=150):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(clean_text)\n",
    "print(\"Total chunks:\", len(chunks))\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(chunks)\n",
    "\n",
    "\n",
    "question = input(\"\\n‚ùì Enter your question: \")\n",
    "\n",
    "query_vector = vectorizer.transform([question])\n",
    "\n",
    "scores = cosine_similarity(query_vector, X)[0]\n",
    "top_k = 3\n",
    "top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "\n",
    "retrieved_chunks = [chunks[i] for i in top_indices]\n",
    "\n",
    "print(\"\\nüîç Retrieved Relevant Sections:\\n\")\n",
    "for i, idx in enumerate(top_indices, 1):\n",
    "    print(f\"[Section {i}]  (Score: {scores[idx]:.4f})\")\n",
    "    print(chunks[idx][:400])\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def simplify_text(text, max_sentences=5):\n",
    "    sentences = text.split(\". \")\n",
    "    return \". \".join(sentences[:max_sentences]) + \".\"\n",
    "\n",
    "combined_text = \" \".join(retrieved_chunks)\n",
    "\n",
    "print(\"\\nü§ñ Simplified Answer:\\n\")\n",
    "print(simplify_text(combined_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b8f868-dac1-4703-9f2e-adc9e6bb1912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml_clean]",
   "language": "python",
   "name": "conda-env-ml_clean-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
